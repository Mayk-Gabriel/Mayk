{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a598618",
   "metadata": {},
   "source": [
    "# PROJETO DE CLASSIFICACAO - COMPARACAO DE ALGORITMOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83da8a6",
   "metadata": {},
   "source": [
    " Aluno: Davi Medeiros \n",
    " Data: 28/11/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58706406",
   "metadata": {},
   "source": [
    "# 1 IMPORTACAO DE BIBLIOTECAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0486900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"PROJETO DE CLASSIFICACAO - CREDIT SCORING\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683431c1",
   "metadata": {},
   "source": [
    "# 2 CRIACAO DA BASE DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78215a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_credit_data():\n",
    "    \"\"\"\n",
    "    Cria dataset simulado de credit scoring com 1000 observacoes\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    data = {\n",
    "        'idade': np.random.randint(18, 70, n_samples),\n",
    "        'renda_mensal': np.random.normal(5000, 2000, n_samples),\n",
    "        'valor_emprestimo': np.random.normal(15000, 8000, n_samples),\n",
    "        'prazo_emprestimo': np.random.randint(6, 60, n_samples),\n",
    "        'historico_credito': np.random.choice(['bom', 'regular', 'ruim'], n_samples, p=[0.6, 0.3, 0.1]),\n",
    "        'emprego_tipo': np.random.choice(['assalariado', 'autonomo', 'empresario', 'desempregado'], n_samples, p=[0.5, 0.3, 0.15, 0.05]),\n",
    "        'tempo_emprego': np.random.uniform(0, 20, n_samples),\n",
    "        'dependentes': np.random.randint(0, 5, n_samples),\n",
    "        'score_serasa': np.random.randint(0, 1000, n_samples)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.DataFrame(data)\n",
    "    \n",
    "    # Criar variavel target (inadimplente: 0 = nao, 1 = sim)\n",
    "    prob_inadimplente = (\n",
    "        (df['historico_credito'] == 'ruim') * 0.6 +\n",
    "        (df['historico_credito'] == 'regular') * 0.3 +\n",
    "        (df['score_serasa'] < 300) * 0.4 +\n",
    "        (df['renda_mensal'] < 2000) * 0.5 +\n",
    "        (df['valor_emprestimo'] / df['renda_mensal'] > 10) * 0.4 +\n",
    "        np.random.normal(0, 0.1, n_samples)\n",
    "    )\n",
    "    \n",
    "    df['inadimplente'] = (prob_inadimplente > 0.5).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Criar dataset\n",
    "df = create_credit_data()\n",
    "print(\"Base de Dados de Credit Scoring\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Distribuicao da variavel target:\")\n",
    "print(df['inadimplente'].value_counts())\n",
    "print(f\"Proporcao: {df['inadimplente'].value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e6ec2",
   "metadata": {},
   "source": [
    "# 3 PRE-PROCESSAMENTO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_credit_data(df):\n",
    "    \"\"\"\n",
    "    Realiza pre-processamento dos dados\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Verificar valores missing\n",
    "    print(\"Valores missing por coluna:\")\n",
    "    print(data.isnull().sum())\n",
    "    \n",
    "    # Separar variaveis preditoras e target\n",
    "    X = data.drop('inadimplente', axis=1)\n",
    "    y = data['inadimplente']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = preprocess_credit_data(df)\n",
    "print(f\"Pre-processamento concluido\")\n",
    "print(f\"Variaveis preditoras: {X.shape[1]}\")\n",
    "print(f\"Target: {y.shape[0]} observacoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf4f50",
   "metadata": {},
   "source": [
    "# 4 CODIFICACAO DE VARIAVEIS CATEGORICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(X):\n",
    "    \"\"\"\n",
    "    Aplica LabelEncoder para variaveis categoricas\n",
    "    \"\"\"\n",
    "    X_encoded = X.copy()\n",
    "    \n",
    "    # LabelEncoder para variaveis categoricas\n",
    "    label_encoders = {}\n",
    "    categorical_columns = ['historico_credito', 'emprego_tipo']\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "        label_encoders[col] = le\n",
    "        print(f\"{col} codificado: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    \n",
    "    return X_encoded, label_encoders\n",
    "\n",
    "X_encoded, label_encoders = encode_categorical_features(X)\n",
    "print(f\"Shape apos codificacao: {X_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe0ecc1",
   "metadata": {},
   "source": [
    "# 5 PADRONIZACAO DE VARIAVEIS NUMERICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a708e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_numeric_features(X):\n",
    "    \"\"\"\n",
    "    Aplica StandardScaler para variaveis numericas\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Colunas numericas para padronizar\n",
    "    numeric_cols = ['idade', 'renda_mensal', 'valor_emprestimo', 'prazo_emprestimo', \n",
    "                   'tempo_emprego', 'dependentes', 'score_serasa']\n",
    "    \n",
    "    X_standardized = X.copy()\n",
    "    X_standardized[numeric_cols] = scaler.fit_transform(X_standardized[numeric_cols])\n",
    "    \n",
    "    return X_standardized, scaler\n",
    "\n",
    "X_standardized, scaler = standardize_numeric_features(X_encoded)\n",
    "print(\"Padronizacao concluida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6868b",
   "metadata": {},
   "source": [
    "# 6 DIVISAO DOS DADOS E SALVAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8eefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir entre treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_standardized, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Divisao dos dados:\")\n",
    "print(f\"Treino: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Teste:  X_test {X_test.shape}, y_test {y_test.shape}\")\n",
    "\n",
    "# Salvar os datasets em arquivos pkl\n",
    "with open('X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open('X_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "with open('y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open('y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "print(\"Arquivos salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc7ae6",
   "metadata": {},
   "source": [
    "# 7 TREINAMENTO DOS MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1709ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir e treinar multiplos algoritmos\n",
    "models = {\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Arvore de Decisao': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
    "    'KNN (k=7)': KNeighborsClassifier(n_neighbors=7)\n",
    "}\n",
    "\n",
    "# Dicionario para armazenar resultados\n",
    "results = {}\n",
    "\n",
    "print(\"Treinando modelos...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Treinando {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"   {name} - Acuracia: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Salvar modelos treinados\n",
    "for name, result in results.items():\n",
    "    with open(f'modelo_{name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")}.pkl', 'wb') as f:\n",
    "        pickle.dump(result['model'], f)\n",
    "\n",
    "print(\"Modelos salvos em arquivos .pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9f4a6",
   "metadata": {},
   "source": [
    "# 8 COMPARACAO DE PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21068582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar relatorio comparativo\n",
    "print(\"COMPARACAO DOS ALGORITMOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Modelo': list(results.keys()),\n",
    "    'Acuracia': [results[name]['accuracy'] for name in results.keys()],\n",
    "    'F1-Score': [results[name]['f1_score'] for name in results.keys()]\n",
    "}).sort_values('Acuracia', ascending=False)\n",
    "\n",
    "print(comparison_df.round(4))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identificar melhor modelo\n",
    "best_model_name = comparison_df.iloc[0]['Modelo']\n",
    "best_accuracy = comparison_df.iloc[0]['Acuracia']\n",
    "best_f1 = comparison_df.iloc[0]['F1-Score']\n",
    "\n",
    "print(f\"MELHOR MODELO: {best_model_name}\")\n",
    "print(f\"   Acuracia: {best_accuracy:.4f}\")\n",
    "print(f\"   F1-Score: {best_f1:.4f}\")\n",
    "\n",
    "# Detalhes do melhor modelo\n",
    "print(f\"Relatorio detalhado do {best_model_name}:\")\n",
    "print(classification_report(y_test, results[best_model_name]['predictions']))\n",
    "\n",
    "# Matriz de confusao do melhor modelo\n",
    "print(f\"Matriz de Confusao - {best_model_name}:\")\n",
    "conf_matrix = confusion_matrix(y_test, results[best_model_name]['predictions'])\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805df14",
   "metadata": {},
   "source": [
    "# 9 VISUALIZACAO DOS RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar estilo dos graficos\n",
    "plt.style.use('default')\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Grafico 1: Comparacao de Acuracia\n",
    "models_names = comparison_df['Modelo']\n",
    "accuracies = comparison_df['Acuracia']\n",
    "colors = ['green' if x == best_model_name else 'skyblue' for x in models_names]\n",
    "\n",
    "ax1.barh(models_names, accuracies, color=colors)\n",
    "ax1.set_xlabel('Acuracia')\n",
    "ax1.set_title('Comparacao de Acuracia dos Modelos')\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "# Grafico 2: Comparacao de F1-Score\n",
    "f1_scores = comparison_df['F1-Score']\n",
    "colors_f1 = ['orange' if x == best_model_name else 'lightcoral' for x in models_names]\n",
    "\n",
    "ax2.barh(models_names, f1_scores, color=colors_f1)\n",
    "ax2.set_xlabel('F1-Score')\n",
    "ax2.set_title('Comparacao de F1-Score dos Modelos')\n",
    "ax2.set_xlim(0, 1)\n",
    "\n",
    "# Grafico 3: Matriz de Confusao do Melhor Modelo\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax3)\n",
    "ax3.set_title(f'Matriz de Confusao - {best_model_name}')\n",
    "ax3.set_ylabel('Verdadeiro')\n",
    "ax3.set_xlabel('Predito')\n",
    "\n",
    "# Grafico 4: Importancia das Features (apenas para Random Forest)\n",
    "if best_model_name == 'Random Forest':\n",
    "    feature_importance = results[best_model_name]['model'].feature_importances_\n",
    "    features = X_train.columns\n",
    "    feature_df = pd.DataFrame({'feature': features, 'importance': feature_importance})\n",
    "    feature_df = feature_df.sort_values('importance', ascending=True)\n",
    "    \n",
    "    ax4.barh(feature_df['feature'], feature_df['importance'])\n",
    "    ax4.set_title('Importancia das Features - Random Forest')\n",
    "    ax4.set_xlabel('Importancia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparacao_modelos.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c993086",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PROJETO CONCLUIDO COM SUCESSO!\")\n",
    "print(\"ARQUIVOS GERADOS:\")\n",
    "print(\"   - X_train.pkl, X_test.pkl, y_train.pkl, y_test.pkl\")\n",
    "print(\"   - modelo_[NOME_DO_MODELO].pkl para cada algoritmo\")\n",
    "print(\"   - comparacao_modelos.png (graficos comparativos)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
